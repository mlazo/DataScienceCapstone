{
    "collab_server" : "",
    "contents" : "## Coursera Data Science Capstone Project\n## Marcia Lazo\n\n\nsuppressPackageStartupMessages(c(\n        library(shinythemes),\n        library(shiny),\n        library(tm),\n        library(stringr),\n        library(markdown),\n        library(stylo),\n        library(dplyr)))\n\n#The data was preprocessed using the code in prepareData.R, and then saved to file\n#It is being loaded here. I used 2% of all of the data as my sample and created the\n#n-grams from that 2%\nuniData <- load(file=\"./data/ngram_sorted_1.RData\")\nunigrams = gramfreq\nbiData <- load(file=\"./data/ngram_sorted_2.RData\")\nbigrams = gramfreq\ntriData <- load(file=\"./data/ngram_sorted_3.RData\")\ntrigrams = gramfreq\nquadData <- load(file=\"./data/ngram_sorted_4.RData\")\nquadgrams = gramfreq\n\n#This function is used to clean up the input text. The texts used to create the n-grams were all cleaned similarly ahead of time\n#so this text needs to be cleaned as well to make sure that a fair comparison of words is done\ncleanTheText<-function(text){\n        \n        clean <- tolower(text)\n        clean <- removePunctuation(clean)\n        clean <- removeNumbers(clean)\n        clean <- str_replace_all(clean, \"[^[:alnum:]]\", \" \")\n        clean <- stripWhitespace(clean)\n\n        return(clean)\n}\n\n#Using txt.to.word.ext from stylo. This package is good for English language tokenization\n#It splits the text into words and leaves contractions such as don't untouched. It also leaves\n#compound word constructs such as daughter-in-law intact as one \"word\"\ncleanInput <- function(text){\n        \n        input <- cleanTheText(text)\n        input <- txt.to.words.ext(input, language=\"English.all\", preserve.case = TRUE)\n        return(input)\n}\n\nPredNextTerm <- function(inStr)\n{\n  inStr <- unlist(strsplit(inStr, split=\" \"));\n  inStrLen <- length(inStr);\n  \n  Found <- FALSE;\n  predNxtTerm <- as.character(\"Nothing entered yet\");\n  if (inStrLen >= 3 & !Found)\n  {\n    # Take the input string and tokenize it, separating tokesn by a single space \n    input <- paste(inStr[(inStrLen-2):inStrLen], collapse=\" \");\n    searchStr <- paste(\"^\",input, sep = \"\");\n    #Find all instances where the final three words from the phrase are the first three words in a quadgram\n    quadgramsTemp <- subset(quadgrams, grepl(searchStr, quadgrams[,1]))\n    # Check to see if any matching records were found\n    if ( length(quadgramsTemp[, 1]) > 1 )\n    {\n      predNxtTerm <- quadgramsTemp[1,1];\n      Found <- TRUE;\n    }\n    quadgramsTemp <- NULL;\n  }\n  # if that failed find all instances where the final two words from the phrase are the first two words in a trigram\n  if (inStrLen >= 2 & !Found){\n    inStr1 <- paste(inStr[(inStrLen-1):inStrLen], collapse=\" \");\n    searchStr <- paste(\"^\",inStr1, sep = \"\"); \n    trigramsTemp <- subset(trigrams, grepl(searchStr, trigrams[,1]))\n    if ( length(trigramsTemp[, 1]) > 1 )\n    {\n      predNxtTerm <- trigramsTemp[1,1];\n      Found <- TRUE;\n    }\n    trigramsTemp <- NULL;\n  }\n  # if that failed find all instances where the final (or only) word from the phrase is the first word in a bigram\n  if (inStrLen >= 1 & !Found){\n    inputStr <- inStr[inStrLen]\n    searchStr <- paste(\"^\", inputStr, sep = \"\")\n    bigramsTemp <- subset(bigrams, grepl(searchStr, bigrams[, 1]))\n    #print(bigramsTemp)\n    # Check to see if any matching record returned\n    if (length(bigramsTemp[, 1]) > 1)\n    {\n      predNxtTerm <- bigramsTemp[1, 1]\n      Found <- TRUE\n    }\n    bigramsTemp <- NULL\n  }\n  if(!Found & inStrLen > 0)\n  {\n    predNxtTerm <- unigrams[1,1];\n  }\n  \n  #nextTerm <- word(predNxtTerm, -1);\n  print(predNxtTerm)\n  if (inStrLen > 0){\n    nextTerm <- word(predNxtTerm, -1);\n    return(nextTerm)\n  } else {\n    nextTerm <- \"\";\n    return(nextTerm);\n  }\n}",
    "created" : 1475637909769.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "3735273938",
    "id" : "1E204F45",
    "lastKnownWriteTime" : 1476039694,
    "last_content_update" : 1476039694623,
    "path" : "~/Documents/DataScienceCoursera/MLazoCapstoneFinalProj/cleanAndPredict.R",
    "project_path" : "cleanAndPredict.R",
    "properties" : {
    },
    "relative_order" : 3,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_source"
}